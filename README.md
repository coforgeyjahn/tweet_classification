The problem I have chosen to research is tweet classification using sentiment analysis. Sentiment analysis of social media, and specifically Twitter (X), is challenging for a few different reasons. First, the character limit of a tweet is much smaller than that of other social media platforms (at only 280 characters). This makes it more difficult to gain context and understanding, thereby increasing the potential for classifiers to make mistakes. Additionally, because Twitter is much more informal than platforms such as LinkedIn, classifiers may encounter different types of slang, which may include words and contexts they have not previously seen. These obstacles make classifying the sentiment of tweets particularly more challenging than classifying the sentiment of posts on other social media platforms. Thus, I have attempted to train a model that performs well given this character limitation and learns slang based on the context and usage of these words.

Sentiment analysis on social media proves useful in determining public perception of various subjects, events, or ideas. The model I have chosen to build uses Natural Language Processing techniques. For example, I compare the differences between classification using a bag of words count vectorizer and classification using a TF-IDF vectorizer. Then, I compare the performance of each of these classifiers using different models, including a logistic regression model and a support vector machine model, and multinomial naive bayes to classify tweets from a test selection.

Literature Review:

In this literature review, I will compare the methodologies, selected datasets, and evaluations of two research papers. The first, "A survey of sentiment analysis in social media. Knowledge and Information Systems," has a more general review of sentiment analysis across several social media platforms. I will refer to this paper as (5), cited in the bibliography. The second, "Like It or Not: A survey of Twitter Sentiment Analysis Methods," focuses on Twitter-specific sentiment analysis. I will refer to this paper as (6), also cited in the bibliography.
In analyzing the preprocessing steps described in both papers, I found that essentially the same steps were taken. These steps include tokenization, removing special characters, and converting all letters to lowercase. Additionally, in paper (6), they remove Twitter-specific tokens such as “#,” “@,” URLs, and usernames.
In terms of feature extraction, paper (5) included a wider variety of feature types such as videos and images, whereas paper (6) only examined the actual text in the tweets and related features such as lexical, syntactic, and semantic features. Both papers compared the differences between using unigrams, bigrams, n-grams, and semantic features such as word embeddings.

When applying machine-learning-based approaches to analyze and categorize social media sentiments, paper (5) found that applying K-nearest-neighbors, Naive Bayes, a Support Vector Machine, and a Random Forest model on a dataset of movie reviews resulted in “KNN, with the help of Information gain feature selection, becoming the best performing method with 96.8% accuracy while the optimum K is 3.” For Twitter-specific sentiment analysis, paper (6) cited that through trying various combinations of unigrams, bigrams, n-grams, Naive Bayes, SVMs, and MaxEnt, “the most effective method was using NB with bigrams as features, which managed to achieve an accuracy of 82.7%.” Both papers used similar metrics to evaluate the performance of the optimal models: accuracy, precision, recall, f1-score, and mean squared error.
Both papers cited similar challenges with Twitter-specific sentiment analysis. A few of these challenges include the use of emoticons, sarcasm or irony detection, the use of informal language, and multilingual sentiment analysis. The extremely vast number of tweets, even in real-time, is stated as another challenge. It is important that in training a sentiment analysis model for categorizing tweets, we cast a wide net to include tweets from all different types of users from all around the world. This can be extremely challenging, as in 2022 it was estimated that there were around 6,000 tweets created per second. This amounts to 500 million tweets per day or 200 billion tweets per year.

Methodology:

The dataset I have chosen consists of 163,000 tweets. There are two columns in the dataset: cleaned text (the actual wording of the tweets with any special characters and some punctuation removed) and category (the sentiment of the tweet, which is classified into three categories: -1 for negative sentiment, 0 for neutral, and 1 for positive sentiment). It is interesting to note that in the initial dataset, some tweets contain certain punctuation such as quotation marks, apostrophes, and even emojis. In my additional cleaning of the text, I will assess whether the emojis need to be classified and determine whether or not I should remove them.
The dataset consists of 35,510 tweets with negative sentiment classification, 55,220 with neutral sentiment, and 72,250 with positive sentiment classification. When I split the data, I will use 20% for the test dataset.

My model design is as follows: I will download my dataset from Kaggle in the form of a CSV file. Then, in cleaning this dataset, I will first select only the rows that contain emojis and store this for potential analysis later. Next, I will handle the case where a tweet has not been assigned a sentiment value by assigning these tweets a neutral sentiment (0). I will only be considering tweets in English for simplicity in this assignment. I’ll then lemmatize the tweets and remove aspects of a tweet that won’t be useful in our classification, such as mentions (tagging another user with the ‘@’ symbol) and the ‘#’ symbol. Due to less than .01% of the tweets containing emojis, I will not need to consider emoji classification.
In my model design, I will first try two different methods of feature extraction and text classification: the TF-IDF method and the Bag of Words method. I will then deploy a logistic
regression and a support vector machine on both and compare the performances in terms of accuracy, precision, recall, and f1-score.


Revised Methodologies:

Since we used Naive Bayes and logistic regression classifiers in part 1, we will now try using two new classifiers - a random forest and Convolutional Neural Network (CNN). For the random forest, I will set maximum features to be 2000 with 70 decision trees (n_estimator=70). I started out by attempting to have 100 decision trees to make my model more robust, but was stuck dealing with a very long computation time. With the random forest and only 10 decision trees, I was able to achieve an accuracy of 77.5%, which is worse than both the Multinomial Naive Bayes and the Logistic Regression. When I expand this to include 70 decision trees, we are able to achieve an accuracy of 83.5%.
For my CNN, I first tokenized and padded the data to make sure all inputs were of the same length. The tuning of the hyperparameters of my CNN presented me with even more of a challenge. First I opted to use a sequential model, which is a linear stack of layers, where each layer has weights that are learned during training. Then I embed my model so that each word in the vocabulary is represented by a dense vector with 100 values. Then I choose to use a Convolutional 1 dimensional vector with a kernel size of 5, meaning that each kernel looks at 5 words at a time. Then I use a dropout value of .5 to prevent overfitting. I add in Global Max Pooling to preserve important features while reducing dimensionality. Lastly, I add a Dense layer with a sigmoid activation function, which is optimal to use for binary classification. Then, I tried to implement early stopping to stop training of the model if the validation loss didn’t improve, but found that this didn’t make a difference in computational complexity or improving metrics. I used Adam (Adaptive Moment Estimation) as my optimizer to update the learning rates and the weights of the biases in different layers of my CNN.


Evaluation of Results:

For my methods of feature extraction, I chose to run both a logistic regression and a multinomial naive bayes classifier on a bag of words text translation and a TF-IDF text translation, resulting in four possible models. The best-performing model was the Bag of Words with Logistic Regression, with an accuracy of 79%.
I tried using an SVM, but I was getting stuck with runtimes taking over an hour and occasionally crashing the program, no matter what kernel I opted to use. This is because an SVM works best on non-text, structured data. The performances of my models, ranked best to worst, were as follows: TF-IDF with Random Forest: 0.83, Bag of Words with Logistic Regression: 0.78, TF-IDF with Logistic Regression: 0.78, Bag of Words with Multinomial Naive Bayes: 0.78, TF-IDF with Multinomial Naive Bayes: 0.66, Convolutional Neural Network: 0.60.
The Random forest turned out to perform the best of all of the methods used with the TF-IDF vectorizer with 2000 maximum features. It was surprising to see how poorly the convolutional
neural network performed, given that this model is more advanced. One possible reason for this lower performance is that we can handle short texts like tweets more effectively by using simpler models that use bag-of-words or TF-IDF vectorizers. The use of these vectorizers are critical in this assignment since they are able to capture word frequency and importance, which is very useful in sentiment analysis. Another thing that I found to be interesting was that the random forest seemed much more responsive to hyperparameter tuning, where finding the correct number of decision trees seemed to be the most important hyperparameter to nail down. I found that with too few decision trees we were not achieving accuracy as good as we could’ve, and too many decision trees resulted in extremely long runtimes. Thus while training our CNN, metrics didn’t get much better or worse as the hyperparameters changed.


Ethical Implications:

There are several potential ethical dilemmas that can arise from using logistic regression or multinomial naive bayes to perform sentiment analysis on a dataset of tweets and tweet classification. Some of these that I will detail include lack of information about the tweets, bias from authors, lack of information about how the initial categorization (-1, 0, 1) was done, and overall tweet quality. Since we are only given the text content of the tweets without any context, this could lead to skewed or biased results, which could be unethical in practice. We are given no information about who authored the tweets, when the data was collected, or if our sample encompasses the tweets of a diverse group of people. This could be problematic in several different ways. Some of these include if our tweets are all pulled from people with the same political views, if the authors had strong biases, if the authors were sponsored by a brand or partnership in creating the tweet, or if the data was collected during a certain period when people felt more strongly about something than normal. An example of this would be if the sample was collected on the USA election night in 2016 (the single day with the most tweets since Twitter was founded). This was a time when the country was very divided, and tensions were extremely heightened. If tweets were collected then, the results would show an overwhelming amount of negative sentiment (-1). If this was used to train our model, it wouldn’t accurately represent the overall sentiment of Twitter content.

We also aren’t given information about how the tweets were assigned the values of -1, 0, 1 in the first place. This could pose potential ethical dilemmas and bias if, say, a human with bias were the one going through and assigning this categorization. Consider the tweet “London is insane!”. Since this tweet doesn’t provide any context, it’s up to the reader’s interpretation of the word “insane” or their personal perception of London to determine if this is a positive, negative, or neutral tweet. Without knowledge of how the dataset was classified, it could be dangerous to train and deploy this model in the real world with a dataset that is potentially full of human bias.
Finally, since Twitter is a very informal social media platform with a character limit, the risk of misclassification is much higher than it might be on other social media platforms. The character limitation leads to less context overall, which can be problematic when using things like TF-IDF, which rely on context. The use of slang or the increased likelihood of misspellings or incorrect grammar can also lead to more complexity in building an accurate classifier. 

In Conclusion while classifying tweets using sentiment analysis presents unique challenges, I found that it is possible to attain pretty reasonable accuracy (78%) by combining a TF-IDF vectorizer or a Bag-of-Words vectorizer with a logistic regression model. I also discovered the importance of choosing the correct classification method, as SVM resulted in extremely long runtimes and Multinomial Naive Bayes had a significantly worse performance. From my revised methodologies, I found that it is probably not optimal to use a sophisticated model like a CNN for something as simple and binary as sentiment analysis of tweets. I found that a random forest with the number of decision trees tuned can achieve the highest accuracy of all (83%) due to its simplicity and the way it is able to capture context.

In this project I learned a lot about the ethical concerns that could play into sentiment analysis of social media posts, and especially with a social media platform that is as informal and as lacking in context as Twitter. In the future I would like to explore this subject further, training a model on a vocabulary of informal and slang language to see if this would be able to perform even better.


References
(1) "LITERATURE REVIEW ON SENTIMENT ANALYSIS OF TWITTER DATA", International Journal of Emerging Technologies and Innovative Research (www.jetir.org | UGC and issn Approved), ISSN:2349-5162, Vol.6, Issue 6, page no. pp177-180, June-2019, Available at : http://www.jetir.org/papers/JETIRCO06035.pdf
(2) “Sentiment Analysis in Tweets: An Assessment Study from Classical to Modern Word Representation Models | Data Mining and Knowledge Discovery.” SpringerLink, https://link.springer.com/article/10.1007/s10618-022-00853-0. Accessed 25 June 2024.
(3) “A Reliable Sentiment Analysis for Classification of Tweets in Social Networks - PMC.” PubMed Central (PMC), https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9742011/. Accessed 25 June 2024.
(4) Medhat, W., Hassan, A., & Korashy, H. (2014). Sentiment analysis algorithms and applications: A survey. Ain Shams Engineering Journal, 5(4), 1093-1113.
(5) Yue, L., Chen, W., Li, X., Zuo, W., & Yin, M. (2019). A survey of sentiment analysis in social media. Knowledge and Information Systems, 60(2), 617-663.
(6) Giachanou, A., & Crestani, F. (2016). Like it or not: A survey of Twitter sentiment analysis methods. ACM Computing Surveys (CSUR), 49(2), 1-41.
(7) Sayce, David. “The Number of Tweets per Day in 2022 - David Sayce.” David Sayce, Paper Gecko Ltd
(8) Chang, Eric. “Complete Guide to Perform Classification of Tweets with SpaCy.” Towards Data Science, Towards Data Science, 25 Nov. 2021, https://towardsdatascience.com/complete-guide-to-perform-classification-of-tweets-with- spacy-e550ee92ca79.
 
