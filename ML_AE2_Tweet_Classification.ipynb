{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e17f822f",
   "metadata": {},
   "source": [
    "# Part 1 - Classification of Tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371919d",
   "metadata": {},
   "source": [
    "In part 1, we first want to read in all of the data from our dataset, which is stored in 'Twitter_Data.csv'. We see that each row of this data has only 2 columns - a cleaned text 'clean_text' and a catgory. Here, the 'clean_text' represents the actual tweet and the category determines whether the tweet is positive, negative, or neutral, with a 1 being positive 0 being neutral and -1 being negative. We'll read in this data to an object named tweets, and then print the counts of the number of positive, neutral, and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d4d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e367791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "tweets = pd.read_csv('Twitter_Data.csv')\n",
    "tweets['clean_text'] = tweets['clean_text'].astype(str).fillna('')\n",
    "tweets['category'] = tweets['category'].fillna(0)  # Replace NaN with a neutral sentiment or appropriate value\n",
    "\n",
    "# Shuffle data in order to ensure randomization\n",
    "tweets = tweets.sample(frac=1)\n",
    "\n",
    "# Get rows with emojis\n",
    "tweets_with_emojis = tweets[tweets['clean_text'].str.contains(r'[\\u263a-\\U0001f645]')]\n",
    "\n",
    "# Initialize lemmatizer and stopwords list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tweets.dropna()\n",
    "\n",
    "def cleaning_and_preprocessing(tweet):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+|[^A-Za-z0-9\\s]', '', tweet)\n",
    "    # Tokenize and lemmatize\n",
    "    words = tweet.split()\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the 'clean_text' column\n",
    "tweets['clean_text'] = tweets['clean_text'].apply(cleaning_and_preprocessing)\n",
    "tweets = tweets[tweets['clean_text'].apply(lambda x: x.strip() != '')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f986ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1834\n"
     ]
    }
   ],
   "source": [
    "print(tweets_with_emojis.shape[0]) # Due to less than .01% of the tweets containing emojis, we will not need to consider emoji classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957d23d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get negative, neutral, and positive tweets from the training dataset\n",
    "negative = tweets.loc[tweets['category'] == -1]\n",
    "neutral = tweets.loc[tweets['category'] == 0]\n",
    "positive = tweets.loc[tweets['category'] == 1]\n",
    "\n",
    "# get count of each new dataframe\n",
    "neg_count = negative.shape[0]\n",
    "neutral_count = neutral.shape[0]\n",
    "pos_count = positive.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c10fc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35509\n",
      "55157\n",
      "72245\n",
      "                                               clean_text  category\n",
      "129605                      narendra modi killed shashtri      -1.0\n",
      "128942  anything big india achieves naamdaars sad doub...      -1.0\n",
      "28644   day chandrababu sir talking kcr sirpm modi sir...      -1.0\n",
      "77301   beef export country increased modi govt muslim...      -1.0\n",
      "33506   want freebie also know modi work change countr...      -1.0\n",
      "...                                                   ...       ...\n",
      "142994  hit nail head arguing modi bhakt like throwing...      -1.0\n",
      "95292   date night modiin dinner meat meet one faves l...      -1.0\n",
      "112317  exit airport gate near ola pickup point big ho...      -1.0\n",
      "20122   dhokla asaduddin owaisi double beef attack mod...      -1.0\n",
      "20169   sir winter gone suggest modi tshirt sale incre...      -1.0\n",
      "\n",
      "[35509 rows x 2 columns]\n",
      "                                               clean_text  category\n",
      "150479  mama bigger congress country narendra modi ass...       0.0\n",
      "9701    modi edge rival digital outreach struck chord ...       0.0\n",
      "109349  signed wellknown people including anand patwar...       0.0\n",
      "133873                                appeal vote modibjp       0.0\n",
      "40080                         suck modis youll get ticket       0.0\n",
      "...                                                   ...       ...\n",
      "118295        tomorrow pidis burrow shouting managed modi       0.0\n",
      "64623   uhm stuff required political well besides modi...       0.0\n",
      "134076    modi 2nd thing india need urgently book written       0.0\n",
      "140619                            intresting read article       0.0\n",
      "139365  modi gujarat nephew working marketing sale req...       0.0\n",
      "\n",
      "[55157 rows x 2 columns]\n",
      "                                               clean_text  category\n",
      "157880  acceptance defeat good health allah sake spew ...       1.0\n",
      "9617                                 one time modi sarkar       1.0\n",
      "113920  one like modi government jus willing respect a...       1.0\n",
      "33861   modi govt shud loud noise mouth comming action...       1.0\n",
      "81025   oil theory negated modi increased oil price pu...       1.0\n",
      "...                                                   ...       ...\n",
      "137404  bhakts batao soldier vote chaukidar let see lo...       1.0\n",
      "73081   cought openly accept fact although intention c...       1.0\n",
      "144903  desh taiyar hai modi sunane live live 1030 exc...       1.0\n",
      "145467  also visit muslim area win confidence must try...       1.0\n",
      "26252   cant fool publicur offer wont able fetch votes...       1.0\n",
      "\n",
      "[72245 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(neg_count)\n",
    "print(neutral_count)\n",
    "print(pos_count)\n",
    "print(negative)\n",
    "print(neutral)\n",
    "print(positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40cdaf1",
   "metadata": {},
   "source": [
    "# Part 2 - Text vectorisation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ba48c",
   "metadata": {},
   "source": [
    "First we will use the TF-IDF method with tokenisation to get a bigram respresentation of our input, and visualize the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46030e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c26d4b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.78      0.55      0.65      7002\n",
      "         0.0       0.71      0.93      0.81     11130\n",
      "         1.0       0.86      0.78      0.82     14451\n",
      "\n",
      "    accuracy                           0.78     32583\n",
      "   macro avg       0.79      0.75      0.76     32583\n",
      "weighted avg       0.79      0.78      0.78     32583\n",
      "\n",
      "0.7819415032378848\n"
     ]
    }
   ],
   "source": [
    "## TF-IDF with Logistic Regression\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1,2), min_df=5, max_df=0.7)\n",
    "\n",
    "# Fit and transform the tweets\n",
    "X = tfidf_vectorizer.fit_transform(tweets['clean_text']).toarray()\n",
    "y = tweets['category']\n",
    "\n",
    "# 20% test, 10% validation, 70% training\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize and train the classifier (Logistic Regression in this case)\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "tfidf_with_logistic_regression_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(tfidf_with_logistic_regression_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2eccd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.83      0.35      0.49      7002\n",
      "         0.0       0.69      0.60      0.64     11130\n",
      "         1.0       0.62      0.86      0.72     14451\n",
      "\n",
      "    accuracy                           0.66     32583\n",
      "   macro avg       0.71      0.60      0.62     32583\n",
      "weighted avg       0.69      0.66      0.64     32583\n",
      "\n",
      "0.6612957677316392\n"
     ]
    }
   ],
   "source": [
    "# TF_IDF with Multinomial NB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1,2), min_df=5, max_df=0.7)  # Limit to top 2000 features for simplicity\n",
    "\n",
    "# Fit and transform the tweets\n",
    "X = tfidf_vectorizer.fit_transform(tweets['clean_text']).toarray()\n",
    "y = tweets['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Multinomial NB classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "tfidf_with_mnb_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(tfidf_with_mnb_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba792969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.78      0.56      0.65      7002\n",
      "         0.0       0.71      0.94      0.81     11130\n",
      "         1.0       0.87      0.77      0.82     14451\n",
      "\n",
      "    accuracy                           0.78     32583\n",
      "   macro avg       0.79      0.76      0.76     32583\n",
      "weighted avg       0.80      0.78      0.78     32583\n",
      "\n",
      "0.7826167019611454\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words with Logistic Regression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=1000)  # Limit to top 1000 features for simplicity\n",
    "\n",
    "# Fit and transform the cleaned text data\n",
    "X = count_vectorizer.fit_transform(tweets['clean_text']).toarray()\n",
    "y = tweets['category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the classifier (Logistic Regression in this case)\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "bow_with_logistic_regression_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(bow_with_logistic_regression_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d134af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.78      0.56      0.65      7002\n",
      "         0.0       0.71      0.94      0.81     11130\n",
      "         1.0       0.87      0.77      0.82     14451\n",
      "\n",
      "    accuracy                           0.78     32583\n",
      "   macro avg       0.79      0.76      0.76     32583\n",
      "weighted avg       0.80      0.78      0.78     32583\n",
      "\n",
      "0.7826167019611454\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words with Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=1000)  # Limit to top 1000 features for simplicity\n",
    "\n",
    "# Fit and transform the cleaned text data\n",
    "X = count_vectorizer.fit_transform(tweets['clean_text']).toarray()\n",
    "y = tweets['category']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Multinomial NB classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "bow_with_mnb_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(bow_with_mnb_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dae953",
   "metadata": {},
   "source": [
    "# Comparing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "286d4383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-idf with Logistic Regression: 0.78\n",
      "Tf-idf with Multinomial Naive Bayes: 0.66\n",
      "Bag of words with Logistic Regression: 0.78\n",
      "Bag of words with Multinomial Naive Bayes: 0.78\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tf-idf with Logistic Regression: {tfidf_with_logistic_regression_accuracy:.2f}\")\n",
    "print(f\"Tf-idf with Multinomial Naive Bayes: {tfidf_with_mnb_accuracy:.2f}\")\n",
    "print(f\"Bag of words with Logistic Regression: {bow_with_logistic_regression_accuracy:.2f}\")\n",
    "print(f\"Bag of words with Multinomial Naive Bayes: {bow_with_mnb_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2a00b",
   "metadata": {},
   "source": [
    "# Part 2 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d07b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8380443789706289\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.83      0.66      0.73      7002\n",
      "         0.0       0.80      0.94      0.87     11130\n",
      "         1.0       0.87      0.85      0.86     14451\n",
      "\n",
      "    accuracy                           0.84     32583\n",
      "   macro avg       0.84      0.81      0.82     32583\n",
      "weighted avg       0.84      0.84      0.83     32583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X = vectorizer.fit_transform(tweets['clean_text']).toarray()\n",
    "y = tweets['category']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=70, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Accuracy: {rf_accuracy}')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc4fd30",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18a9009f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 21ms/step - accuracy: 0.4476 - loss: -52.6002 - val_accuracy: 0.5633 - val_loss: -2322.3687\n",
      "Epoch 2/8\n",
      "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 21ms/step - accuracy: 0.5625 - loss: -11260.3457 - val_accuracy: 0.5873 - val_loss: -80175.5859\n",
      "Epoch 3/8\n",
      "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 20ms/step - accuracy: 0.5758 - loss: -166387.4375 - val_accuracy: 0.5881 - val_loss: -544302.3125\n",
      "Epoch 4/8\n",
      "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 20ms/step - accuracy: 0.5861 - loss: -870354.3750 - val_accuracy: 0.5889 - val_loss: -2000135.0000\n",
      "Epoch 5/8\n",
      "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 20ms/step - accuracy: 0.5886 - loss: -2900356.0000 - val_accuracy: 0.5972 - val_loss: -5384489.5000\n",
      "Epoch 6/8\n",
      "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 20ms/step - accuracy: 0.5909 - loss: -7284564.5000 - val_accuracy: 0.5937 - val_loss: -11936950.0000\n",
      "Epoch 7/8\n",
      "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 21ms/step - accuracy: 0.5871 - loss: -15546888.0000 - val_accuracy: 0.5945 - val_loss: -23207250.0000\n",
      "Epoch 8/8\n",
      "\u001b[1m1146/1146\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 23ms/step - accuracy: 0.5942 - loss: -29263594.0000 - val_accuracy: 0.5940 - val_loss: -41067584.0000\n",
      "\u001b[1m510/510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.00      0.00      0.00      3498\n",
      "         0.0       0.46      1.00      0.63      5647\n",
      "         1.0       0.98      0.57      0.72      7147\n",
      "\n",
      "    accuracy                           0.59     16292\n",
      "   macro avg       0.48      0.52      0.45     16292\n",
      "weighted avg       0.59      0.59      0.53     16292\n",
      "\n",
      "\u001b[1m 55/510\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5833 - loss: -41324176.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/coforgeyjahn/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/coforgeyjahn/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/coforgeyjahn/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m510/510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5917 - loss: -42557760.0000\n",
      "Accuracy: 0.5940338969230652\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X = tweets['clean_text']\n",
    "y = tweets['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Tokenize, convert to sequences, and pad the sequences \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = max(len(seq) for seq in X_train_seq)\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Load pretrained GloVe embeddings\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix]),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    epochs=8,\n",
    "    batch_size=128,\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_padded)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, cnn_accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Accuracy: {cnn_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2be579",
   "metadata": {},
   "source": [
    "# Comparing all accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ced1eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with Logistic Regression: 0.78\n",
      "Tf-IDF with Multinomial Naive Bayes: 0.66\n",
      "Bag of words with Logistic Regression: 0.78\n",
      "Bag of words with Multinomial Naive Bayes: 0.78\n",
      "TF-IDF with Random Forest: 0.84\n",
      "CNN: 0.60\n"
     ]
    }
   ],
   "source": [
    "print(f\"TF-IDF with Logistic Regression: {tfidf_with_logistic_regression_accuracy:.2f}\")\n",
    "print(f\"Tf-IDF with Multinomial Naive Bayes: {tfidf_with_mnb_accuracy:.2f}\")\n",
    "print(f\"Bag of words with Logistic Regression: {bow_with_logistic_regression_accuracy:.2f}\")\n",
    "print(f\"Bag of words with Multinomial Naive Bayes: {bow_with_mnb_accuracy:.2f}\")\n",
    "print(f\"TF-IDF with Random Forest: {rf_accuracy:.2f}\")\n",
    "print(f\"CNN: {cnn_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec2adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
